---
title: "A Zoom Into Data-Related Jobs"
output: html_notebook
---




**Name**: Samaher Brahem 

**Course**: Coding for DS & DM - R Module 

**Professor**: Giancarlo Manzi


### **1. Introduction**

This study aims to analyze salary differences among Data Scientists and other Data-related roles across different countries. Utilizing a dataset containing diverse employment types, experience levels, and company sizes, the study intends to conduct statistical analyses and visualizations using R. The objective is to identify potential salary disparities and trends across various global contexts. The study aims to shed light on nuanced salary patterns within the data science domain, offering insights into the variations of compensation among data-related roles on a global scale.

### **2. Data Used**

The *Salary of Data Scientists* dataset will be used for this analysis. It is made available on [Kaggle](https://www.kaggle.com/datasets/piyushborhade/salary-of-data-scientists) by the user, Piyush Borhade.

#### **2.1 Features of the dataset**:

- `work_year`: The year the salary was paid.

- `experience_level`: The level of work experience during the year

- `employment_type`: the type of employment for the role

- `job_title`: The job in which you worked during the year.

- `salary`: The total gross amount of salary paid.

- `salary_currency`: The currency of the salary paid as an ISO 4217 currency code.

- `salary_in_usd`: The salary in USD

- `employee_residence`: the country of primary residence of the employee during the employment year as an ISO 3166 country code.

- `remote_ratio`: The total amount of work done remotely

- `company_location`: The country of headquarters of the employer or contracting branch

- `company_size`: The average number of people who worked for the company during the year


#### **2.2 Limits of the dataset**:

##### **2.2.1 No Shared Sources**:

The information about how the data was collected and who collected it isn't clear. This lack of clarity raises doubts about how dependable the data is.

##### **2.2.2 Geographical Bias**:


The dataset primarily comprises data from the United States, resulting in an imbalance where other countries are underrepresented. This skew might limit the depth and accuracy of analyses, potentially leading to an overemphasis on American-centric insights and a lack of comprehensive understanding regarding other regions or global perspectives.


### **3. Data Processing**

#### **3. 1 Installing packages and opening libraries:**

- tidyverse

```{r}
library(tidyverse)
library(ggplot2)
library(dplyr)
library(sf)
library(rnaturalearth)

```

#### **3.2 Importing the dataset:**

```{r}
# Importing the dataset from a local path in my computer
data <- read.csv2("C:/Users/Samaher/Documents/DSE_R_Project/data.csv")

# Converting it into a tibble
data <- as.tibble(data)

```


#### **3.3 Previewing the dataset:**

```{r}
# Understanding the structure of the dataset
str(data)
```

```{r}
# Displaying a sample of the data
head(data)
```
Now, let's check the dimensions of the dataset. 

```{r}
# Checking the dimensions of the dataset
dim(data)
```
The dataset has 3753 rows and 12 columns. This means we have 12 variables and 3753 observations. 

#### **3.4 Cleaning and formatting the dataset:**

##### **3.4.1 Experience Level:**

There are 4 experience levels in this dataset:

- EN = Entry Level

- MI = Mid-Level

- SE = Senior

- EX = Executive


```{r}
# Checking how many experience levels are included in this dataset
table(data$experience_level)
```
Let's make those levels understandable by anyone who reads them!

```{r}
# Replacing abbreviations with full descriptions in the 'experience_level' column
data$experience_level <- ifelse(data$experience_level == "EN", "entry level",
                           ifelse(data$experience_level == "EX", "executive",
                           ifelse(data$experience_level == "MI", "mid-level",
                           ifelse(data$experience_level == "SE", "senior", data$experience_level))))

# Checking the updated 'experience_level' column
table(data$experience_level)
```

##### **3.4.2 Employment Type:**

There are 4 employment types in this dataset:

- FT = full-time

- PT = part-time

- CT = contract

- FL = freelance

```{r}
table(data$employment_type)
```
I will replace those abbreviations with the full word for better readability.

```{r}
# Replacing abbreviations with full descriptions in the 'employment_type' column
data$employment_type <- ifelse(data$employment_type == "CT", "contract",
                               ifelse(data$employment_type == "FL", "freelance",
                                      ifelse(data$employment_type == "FT", "full-time",
                                             ifelse(data$employment_type == "PT", "part-time", data$employment_type))))

# Checking the updated 'employment_type' column
table(data$employment_type)
```

##### **3.4.3 Salaries:**

When it comes to salaries, we can see that there are 3 columns: 
1. `salary`
2. `salary_currency`
3. `salary_in_usd` 

The salaries, stored in `salary` column, are collected in local currencies, mentioned in `salary_currency` column. Then, all those salaries were converted to USD and stored in `salary_in_usd`. 

```{r}
table(data$salary_currency)
```

For the purpose of this study, it would be better to have the salaries in EUR. That's why I will replace the `salary_in_usd` column with `salary_in_eur` and convert the salaries to EUR. I will also remove the `salary` and `salary_currecny` columns since they are no longer needed. 

The exchange rate for the conversion from USD to EUR is as follows:

1 United States Dollar equals
0.92 Euro

P.S. The date of this conversion is: Dec 16, 09:12 UTC


```{r}
# Given exchange rate
exchange_rate <- 0.92

# Converting USD salaries to EUR
data$salary_in_usd <- data$salary_in_usd * exchange_rate

# Changing column name to better reflect this change
names(data)[names(data) == "salary_in_usd"] <- "salary_in_eur"
```

```{r}
# Removing the salary and salary_currency columns
data <- subset(data, select = -c(salary, salary_currency))
```

##### **3.4.4 Countries:**

###### **3.4.4.1 Company Location:**

The column `company_location` reflects the countries where the companies are established. Those countries are written as abbreviations. While some country abbreviations are widely known, there are others that are less known. 

```{r}
table(data$company_location)
```


To ensure clarity, I decided to replace the abbreviations with the full country names.

```{r}
# Defining a mapping of country codes to country names for company locations
cl_country_mapping <- c(
  AE = "United Arab Emirates",
  AL = "Albania",
  AM = "Armenia",
  AR = "Argentina",
  AS = "American Samoa",
  AT = "Austria",
  AU = "Australia",
  BA = "Bosnia and Herzegovina",
  BE = "Belgium",
  BO = "Bolivia",
  BR = "Brazil",
  BS = "Bahamas",
  CA = "Canada",
  CF = "Central African Republic",
  CH = "Switzerland",
  CL = "Chile",
  CN = "China",
  CO = "Colombia",
  CR = "Costa Rica",
  CZ = "Czech Republic",
  DE = "Germany",
  DK = "Denmark",
  DZ = "Algeria",
  EE = "Estonia",
  EG = "Egypt",
  ES = "Spain",
  FI = "Finland",
  FR = "France",
  GB = "United Kingdom",
  GH = "Ghana",
  GR = "Greece",
  HK = "Hong Kong",
  HN = "Honduras",
  HR = "Croatia",
  HU = "Hungary",
  ID = "Indonesia",
  IE = "Ireland",
  IN = "India",
  IQ = "Iraq",
  IR = "Iran",
  IT = "Italy",
  JP = "Japan",
  KE = "Kenya",
  LT = "Lithuania",
  LU = "Luxembourg",
  LV = "Latvia",
  MA = "Morocco",
  MD = "Moldova",
  MK = "North Macedonia",
  MT = "Malta",
  MX = "Mexico",
  MY = "Malaysia",
  NG = "Nigeria",
  NL = "Netherlands",
  NZ = "New Zealand",
  PH = "Philippines",
  PK = "Pakistan",
  PL = "Poland",
  PR = "Puerto Rico",
  PT = "Portugal",
  RO = "Romania",
  RU = "Russia",
  SE = "Sweden",
  SG = "Singapore",
  SI = "Slovenia",
  SK = "Slovakia",
  TH = "Thailand",
  TR = "Turkey",
  UA = "Ukraine",
  US = "United States",
  VN = "Vietnam"
)

# Replacing values in the 'company_location' column using the mapping created above
data$company_location <- cl_country_mapping[data$company_location]

```


###### **3.4.4.2 Employee Residence:**

Now, we'll do the same process for `employee_residence` column. 

```{r}
#Checking which countries are included
table(data$employee_residence)
```
```{r}
# Defining a mapping of country codes to country names for employee residence

er_country_mapping <- c(
  AE = "United Arab Emirates",
  AM = "Armenia",
  AR = "Argentina",
  AS = "American Samoa",
  AT = "Austria",
  AU = "Australia",
  BA = "Bosnia and Herzegovina",
  BE = "Belgium",
  BG = "Bulgaria",
  BO = "Bolivia",
  BR = "Brazil",
  CA = "Canada",
  CF = "Central African Republic",
  CH = "Switzerland",
  CL = "Chile",
  CN = "China",
  CO = "Colombia",
  CR = "Costa Rica",
  CY = "Cyprus",
  CZ = "Czech Republic",
  DE = "Germany",
  DK = "Denmark",
  DO = "Dominican Republic",
  DZ = "Algeria",
  EE = "Estonia",
  EG = "Egypt",
  ES = "Spain",
  FI = "Finland",
  FR = "France",
  GB = "United Kingdom",
  GH = "Ghana",
  GR = "Greece",
  HK = "Hong Kong",
  HN = "Honduras",
  HR = "Croatia",
  HU = "Hungary",
  ID = "Indonesia",
  IE = "Ireland",
  IN = "India",
  IQ = "Iraq",
  IR = "Iran",
  IT = "Italy",
  JE = "Jersey",
  JP = "Japan",
  KE = "Kenya",
  KW = "Kuwait",
  LT = "Lithuania",
  LU = "Luxembourg",
  LV = "Latvia",
  MA = "Morocco",
  MD = "Moldova",
  MK = "North Macedonia",
  MT = "Malta",
  MX = "Mexico",
  MY = "Malaysia",
  NG = "Nigeria",
  NL = "Netherlands",
  NZ = "New Zealand",
  PH = "Philippines",
  PK = "Pakistan",
  PL = "Poland",
  PR = "Puerto Rico",
  PT = "Portugal",
  RO = "Romania",
  RS = "Serbia",
  RU = "Russia",
  SE = "Sweden",
  SG = "Singapore",
  SI = "Slovenia",
  SK = "Slovakia",
  TH = "Thailand",
  TN = "Tunisia",
  TR = "Turkey",
  UA = "Ukraine",
  US = "United States",
  UZ = "Uzbekistan",
  VN = "Vietnam"
)

# Replacing values in the 'employee_residence' column using the mapping created above
data$employee_residence <- er_country_mapping[data$employee_residence]

```

##### **3.4.5 Checking for Missing Values:**

After cleaning and formatting the data, I want to make sure there are no null values.

```{r}
# Making sure there are no missing values 
any(is.na(data))
```

The outcome is `FALSE`, so there no null values to deal with. Let's move to the fun part!


### **4. Data Analysis**

#### **4.1 Exploratory Data Analysis (EDA)**

##### **4.1.1 Overview and Summary Statistics**

In this section, we will be examining the dataset's basic characteristics and computing summary statistics for better comprehension. 

###### **4.1.1.1 Overview**

Let's check again the dimensions and structure of the dataset after the the data processing phase. 

```{r}
dim(data)
```
The dataset now has 3753 rows (observations) and 10 columns (variables).

Let's check the columns' names and the structure of the processed dataset.

```{r}
names(data)
```
```{r}
str(data)
```


###### **4.1.1.2 Summary Statistics**

In this section, let's calculate summary statistics like mean, median, standard deviation, minimum, maximum, and quartiles for numerical columns (`salary_in_eur`, `remote_ratio`).

```{r}
# Checking the summary statistics for the salary variable
summary(data$salary_in_eur)
```
These statistics provide insights into the distribution of total gross salaries within the dataset. The minimum salary recorded is €4,721, while the maximum is €414,000. The median salary of €124,200 indicates that half of the recorded total gross salaries fall below this value and the other half above it. The mean salary (€126,499) is slightly higher than the median, indicating potential influence from higher salaries within the dataset. The interquartile range (IQR), between the 25th and 75th percentiles, spans from €87,400 to €161,000, encapsulating the middle 50% of the total gross salary data.


```{r}
# Checking the summary statistics for the remote ratio variable
summary(data$remote_ratio)
```
This shows that most of the values are concentrated towards the lower end, as indicated by the median and the 1st quartile both being 0.00. However, 25% of the data involves entirely remote positions. This means there's a mix of roles, from no remote work to completely remote setups, in the dataset.

##### **4.1.2 Univariate Analysis**

###### **4.1.2.1 Numerical Variables**

**A. Salary Distribution**


```{r}
ggplot(data, aes(x = salary_in_eur)) +
    geom_density(fill = "#5bc8af", color = "#202060") +
    labs(title = "Density Plot of Salary in EUR", x = "Salary in EUR", y = "Density") +
    scale_x_continuous(labels = scales::comma, breaks = seq(0, 500000, by = 100000)) # Modifying the x-axis scale to display breaks at intervals of 100,000 with comma formatting

```
The density plot for the `salary_in_eur` variable reaffirms the statistical observations derived from the summary statistics. It visually depicts a right-skewed distribution, indicating that the majority of salaries are concentrated on the lower end of the spectrum. There's a notable peak at the lower salary values, with a gradual decrease in density as salaries increase. While the plot indicates a rightward skewness, it also showcases a few instances of higher salaries, although these are notably less frequent in the dataset.

**B. Remote Ratio Distribution**

```{r}
ggplot(data, aes(x = remote_ratio)) +
    geom_density(fill = "#5bc8af", color = "#202060") +
    labs(title = "Density Plot of Remote Ratio", x = "Remote Ratio", y = "Density")

```

We can see that there are peaks at both ends (0% and 100%). The density plot represents a bimodal distribution. The plot highlights a high density near 0% and 100%, signifying two prominent modes in the dataset, indicating a significant number of individuals either work fully remotely or fully on-site, with a relatively smaller portion having a moderate remote work percentage around 50%.

###### **4.1.2.2 Categorical Variables**

**A. Company Size**


```{r}

# Creating a table of company sizes and their frequency
company_sizes_freq <- table(data$company_size)

# Converting the table to a data frame
company_sizes_df <- data.frame(company_size = names(company_sizes_freq), frequency = as.numeric(company_sizes_freq))

# Defining a custom color palette (this step is optional, but, they're my favorite colors)
custom_palette <- c("#5bc8af","#202060", "#b030b0","#6c91bf", "#602080")

# Creating the pie chart for company sizes with percentages
ggplot(company_sizes_df, aes(x = "", y = frequency, fill = company_size)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y", start = 0) +
  labs(title = "Company Size Distribution", fill = "Company Size") +
  theme_void() +
  scale_fill_manual(values = custom_palette) +
  geom_text(aes(label = paste0(round((frequency/sum(frequency)) * 100), "%")),
            position = position_stack(vjust = 0.5), color = "white" )

```

The data is composed mainly of medium sized companies with a percentage of 84%. In the 2nd place comes large companies with a percetage of 12%. Small companies represent the smallest category with only 4%.

**B. Work Year**

Let's do the same for the `work_year` variable.

```{r}
# Creating a table of work years and their frequency
work_years_freq <- table(data$work_year)

# Converting the table to a data frame
work_years_df <- data.frame(work_year = names(work_years_freq), frequency = as.numeric(work_years_freq))

# Creating the pie chart for work years with percentages
ggplot(work_years_df, aes(x = "", y = frequency, fill = work_year)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y", start = 0) +
  labs(title = "Work Year Distribution", fill = "Work Year") +
  theme_void() +
  scale_fill_manual(values = custom_palette) +
  geom_text(aes(label = paste0(round((frequency/sum(frequency)) * 100), "%")),
            position = position_stack(vjust = 0.5), color = "white" ) 

```
This pie chart depicts the dominance of 2023 and 2022 in the dataset compared to 2021 and 2020. 2023 has the most significant portion with 48%. 2022 follows with 44%. 2021 and 2020 have a relatively small representation in this data with the percentages 6% and 2% respectively.

**C. Experience Level**

```{r}
# Creating a table of experience levels and their frequency
experience_levels_freq <- table(data$experience_level)

# Converting the table to a data frame
experience_levels_df <- data.frame(experience_level = names(experience_levels_freq), frequency = as.numeric(experience_levels_freq))

# Creating the pie chart for experience levels with percentages
ggplot(experience_levels_df, aes(x = "", y = frequency, fill = experience_level)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y", start = 0) +
  labs(title = "Experience Level Distribution", fill = "Experience Level") +
  theme_void() +
  scale_fill_manual(values = custom_palette) +
  geom_text(aes(label = paste0(round((frequency/sum(frequency)) * 100), "%")),
            position = position_stack(vjust = 0.5), color = "white" )

```
The pie chart shows that most individuals, around 67%, are in senior positions. About 21% are in middle-level roles, while entry-level positions make up 9%. Executive roles are the smallest group, just 3% of the dataset.

**D. Employment Type**

```{r}
# Creating a table of employment types and their frequency
employment_types_freq <- table(data$employment_type)

# Converting the table to a data frame
employment_types_df <- data.frame(employment_type = names(employment_types_freq), frequency = as.numeric(employment_types_freq))

# Creating the pie chart for employment types with percentages
ggplot(employment_types_df, aes(x = "", y = frequency, fill = employment_type)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y", start = 0) +
  labs(title = "Employment Type Distribution", fill = "Employment Type") +
  theme_void() +
  scale_fill_manual(values = custom_palette) +
  geom_text(aes(label = paste0(round((frequency/sum(frequency)) * 100), "%")),
            position = position_stack(vjust = 0.5), color = "white" )

```
The majority of individuals, constituting approximately 99%, are categorized as "Full-time" employees. Now, let's dig a bit deeper to see the percentages of the other categories.

```{r}
# Calculating percentages
employment_types_df$percentage <- round((employment_types_df$frequency / sum(employment_types_df$frequency)) * 100, 2)

employment_types_df
```

This clarifies that part-time roles come in second with 0.45%. Lastly, we have both contract and freelance roles with 0.27% each.

**E. Company Location**

In this section, we will explore where the companies present in this dataset have their headquarters. For this purpose, I chose to visualize the countries on a map. Instead of using a linear scale, I will use a logarithmic scale for the frequency values. The logarithmic scaling helps highlight the distribution more evenly across the map, emphasizing the presence of multiple countries while downplaying the overwhelming frequency of the United States, enabling a clearer visual comparison of the dataset's geographic spread.

```{r}
# Creating a df that has the company locations along with their freq
company_location_df <- data %>%
  group_by(company_location) %>%
  summarise(frequency = n())

# Getting world map data
world <- ne_countries(scale = "medium", returnclass = "sf")

# Merging company_location_df with world map data
map_data <- merge(world, company_location_df, by.x = "name", by.y = "company_location", all.x = TRUE)

# Plotting
ggplot() +
  geom_sf(data = map_data, aes(fill = log(frequency + 1))) +
  scale_fill_gradient(low = "#5bc8af", high = "#202060") +
  theme_minimal()


```
The United States stands out as the most prominent location, reflecting a high occurrence frequency, followed by Canada, Spain, and several other countries that also have notable but comparatively lower representation.

**F. Employee Residence**

Let's do the same for `employee_residence`.

```{r}
# Creating a data frame with employee residence locations and their frequency
employee_residence_df <- data %>%
  group_by(employee_residence) %>%
  summarise(frequency = n())

# Getting world map data
world <- ne_countries(scale = "medium", returnclass = "sf")

# Merging employee_residence_df with world map data
map_data_residence <- merge(world, employee_residence_df, by.x = "name", by.y = "employee_residence", all.x = TRUE)

# Plotting
ggplot() +
  geom_sf(data = map_data_residence, aes(fill = log(frequency + 1))) +
  scale_fill_gradient(low = "#5bc8af", high = "#202060") +
  theme_minimal()

```
Looking at the distribution, it's apparent that the United States has the highest frequency, indicated by the darkest shade on the map. It's followed by Canada, Germany, the United Kingdom, and France, showing relatively high frequencies as well. The lighter shades across various countries denote lower frequencies. Countries such as Tunisia, Algeria, Iraq, Kuwait, American Samoa, Armenia, Bulgaria, Cyprus, Dominican Republic, Iran, Moldova, North Macedonia, Slovakia, and Uzbekistan have the lightest shades, representing the lowest frequency of employee residences.

**G. Job Titles**

In this section, we will see what are the 10 Job Titles present in this dataset.

```{r}
# Creating a table of job titles and their frequency
job_titles_freq <- table(data$job_title)

# Converting the table to a data frame and sort by frequency
job_titles_df <- data.frame(job_title = names(job_titles_freq), frequency = as.numeric(job_titles_freq))
job_titles_df <- job_titles_df[order(job_titles_df$frequency, decreasing = TRUE), ]

# Calculating percentages
job_titles_df$percentage <- (job_titles_df$frequency / sum(job_titles_df$frequency)) * 100

# Selecting the top 10 most occurring job titles
top_10_jobs <- head(job_titles_df, 10)


# Creating a bar plot for the top 10 job titles with percentages on the y-axis
ggplot(top_10_jobs, aes(x = reorder(job_title, -frequency), y = percentage)) +
  geom_bar(stat = "identity", fill = "#5bc8af") +
  labs(title = "Top 10 Most Occurring Job Titles", x = "Job Titles", y = "Percentage") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The most dominant roles include "Data Engineer," "Data Scientist," and "Data Analyst," constituting a significant portion of the dataset's job titles. Additionally, positions like "Machine Learning Engineer," "Analytics Engineer," "Data Architect," and roles related to research, such as "Research Scientist" and "Research Engineer," also hold notable but comparatively smaller percentages.


##### **4.1.3 Bivariate Analysis**

#### **4.2 Further Analysis (Based on EDA Findings)**


### **5. Conclusion**

